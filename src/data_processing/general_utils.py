import geopandas as gpd
import pandas as pd
from loguru import logger
from tqdm.auto import tqdm

from src.data_processing.gee import gee_utils


def generate_locations_with_dates_df(
    df, start_date, end_date, id_col="id", date_col="date"
):
    # We create a dummy date column just so we can use the ffill technique to construct one row for each date per station.
    df = df.copy()
    df[date_col] = pd.to_datetime(start_date, format="%Y-%m-%d")
    df = (
        df.groupby([id_col])
        .apply(
            lambda x: x.set_index(date_col)
            .reindex(pd.date_range(start=start_date, end=end_date))
            .ffill()
            .rename_axis(date_col)
            .reset_index()
        )
        .droplevel(id_col)
    )
    df[date_col] = df[date_col].dt.date
    return df


def collect_gee_datasets(gee_datasets, start_date, end_date, locations_df, id_col="id"):
    gee_dfs = {}
    for gee_index, gee_dataset in enumerate(gee_datasets):

        logger.info(
            f"Collecting GEE data ({gee_index+1} / {len(gee_datasets)}): {gee_dataset}"
        )

        collection_id = gee_dataset["collection_id"]
        bands = gee_dataset["bands"]
        preprocessors = gee_dataset["preprocessors"]

        # For recording all dfs before concatenating later on
        all_dfs = []

        # Iterate through stations
        for index, location in tqdm(locations_df.iterrows(), total=len(locations_df)):
            # Generate station data
            station_gee_values_df = gee_utils.generate_aoi_tile_data(
                collection_id,
                start_date,
                end_date,
                location.latitude,
                location.longitude,
                bands=bands,
                cloud_filter=False,
            )
            # Set the ID so we can join back the data later on
            station_gee_values_df[id_col] = location[id_col]

            # Pre-process
            for preprocessor, params in preprocessors:
                station_gee_values_df = preprocessor(station_gee_values_df, params)

            # Add to main df
            all_dfs.append(station_gee_values_df)

        gee_dfs[collection_id] = pd.concat(all_dfs, axis=0, ignore_index=True)

    return gee_dfs


def join_datasets(
    locations_df,
    start_date,
    end_date,
    gee_dfs,
    hrsl_df,
    id_col,
    date_col="date",
    ground_truth_df=None,
    admin_bounds_gdf=None,
):
    # Create DF with locations + start_date, end_date
    base_df = generate_locations_with_dates_df(
        locations_df, start_date, end_date, id_col=id_col, date_col=date_col
    )

    # Merge HRSL
    # HRSL is a slow-moving feature, and so does not change depending on the date.
    base_df = base_df.merge(hrsl_df, on=[id_col], how="left")

    # Merge GEE dfs
    for _, gee_df in gee_dfs.items():
        base_df = base_df.merge(gee_df, on=[id_col, date_col], how="left")

    # Ground Truth
    if ground_truth_df is not None:
        base_df = base_df.merge(ground_truth_df, on=[id_col, date_col], how="left")

    # Admin Bounds
    if admin_bounds_gdf is not None:
        base_df = gpd.GeoDataFrame(
            base_df,
            geometry=gpd.points_from_xy(base_df["longitude"], base_df["latitude"]),
            crs="EPSG:4326",
        )
        # First, we want to retain only the unique columns in the right GDF to avoid duplicate columns (and consequent column renames).
        cols_unique_to_adm_bounds = admin_bounds_gdf.columns.difference(
            base_df.columns
        ).tolist() + ["geometry"]
        admin_bounds_gdf = admin_bounds_gdf[cols_unique_to_adm_bounds]

        # Perform the actual spatial join
        base_df = gpd.sjoin(base_df, admin_bounds_gdf, predicate="within", how="left")

        # Drop unnecessary columns generated by the sjoin
        base_df.drop(["index_right"], axis=1, inplace=True)

    # Sorting
    base_df = base_df.sort_values(by=[id_col, date_col])

    return base_df
